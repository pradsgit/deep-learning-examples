{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemenatation of YOLO from this paper https://arxiv.org/pdf/1506.02640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.models import googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In YOLO paper, the network consists of 20 conv layers that is pretrained on ImageNet\n",
    "1000-class competition dataset for a week and used this pre-trained network as a backbone.\n",
    "average-pooling layer and an FC layer. why does YOLO use a pre-trained backbone?\n",
    "\n",
    "This pre-trained model is used for detection task. This model is augmented with few other\n",
    "layers for better performance. The final model used for detection is exteded from the pre-trained \n",
    "model by adding four conv layers and two fully connected layers with randomly initialized\n",
    "weights. Also, a typical resolution for the pre-trained models is 224x224 pixels but \n",
    "detection often requires fine-grained visual information so we increase the input resolution of the network\n",
    "from 224 × 224 to 448 × 448.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"device found {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"gpu not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "root='./'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = VOCDetection(root=root, year=\"2012\", image_set=\"train\", download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine conv layer, leaky relu and layer norm\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(layer, N):\n",
    "    return [layer for _ in range(N)]\n",
    "\n",
    "# build lots of CNNs from scratch!!!\n",
    "# (out_channels, kernel_size, stride, padding)\n",
    "\n",
    "def get_custom_model(arch):\n",
    "    in_channels = 3\n",
    "    layers = []\n",
    "\n",
    "    for layer in arch:\n",
    "        if layer == \"M\":\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        elif isinstance(layer, list):\n",
    "            layer_len = len(layer)\n",
    "            num_clones = layer[layer_len - 1] # number of clones to repeat\n",
    "            out_channels, kernel_size, stride, padding = layer[0]\n",
    "            layer1 = ConvLayer(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            in_channels = out_channels\n",
    "            out_channels, kernel_size, stride, padding = layer[1]\n",
    "            layer2 = ConvLayer(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            in_channels = out_channels\n",
    "            for _ in range(num_clones):\n",
    "                layers.extend([layer1, layer2])\n",
    "        else:\n",
    "            out_channels, kernel_size, stride, padding = layer\n",
    "            layers.append(ConvLayer(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            in_channels = out_channels\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture config from image in the paper \n",
    "CUSTOM_ARCH = [\n",
    "    # input shape is 3x448x448\n",
    "    (64, 7, 2, 3),\n",
    "    # input shape is 64x224x224\n",
    "    \"M\",\n",
    "    # input shape is 64x112x112\n",
    "    (192, 3, 1, 1),\n",
    "    # input shape is 192x112x112\n",
    "    \"M\",\n",
    "    # input shape is 192x56x56\n",
    "    (128, 1, 1, 0),\n",
    "    # input shape is 128x56x56\n",
    "    (256, 3, 1, 1),\n",
    "    # input shape is 256x56x56\n",
    "    (256, 1, 1, 0),\n",
    "    # input shape is 256x56x56\n",
    "    (512, 3, 1, 1),\n",
    "    # input shape is 512x56x56\n",
    "    \"M\",\n",
    "    # input shape is 512x28x28\n",
    "    [(256, 1, 1, 0), (512, 3, 1, 1), 4],\n",
    "    # input shape is 512x28x28\n",
    "    (512, 1, 1, 0),\n",
    "    # input shape is 512x28x28\n",
    "    (1024, 3, 1, 1),\n",
    "    # iput shape is 1024x28x28\n",
    "    \"M\",\n",
    "    # input shape is 1024x14x14\n",
    "    [(512, 1, 1, 0), (1024, 3, 1, 1), 2],\n",
    "    # input shape is 1024x14x14\n",
    "    (1024, 3, 1, 1),\n",
    "    # input shape is 1024x14x14\n",
    "    (1024, 3, 2, 1),\n",
    "    # input shape is 1024x7x7\n",
    "    (1024, 3, 1, 1),\n",
    "    (1024, 3, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, grid_size=(7, 7), num_bnb_boxes=2, num_classes=20, use_pretrained=True):\n",
    "        super(YOLOv1, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_bnb_boxes = num_bnb_boxes\n",
    "        self.num_classes = num_classes\n",
    "        if use_pretrained:\n",
    "            print(\"loading pre-trained googlenet\")\n",
    "            self.backbone = googlenet(weights='DEFAULT')\n",
    "            print(\"backbone loaded\")\n",
    "        else:\n",
    "            print(\"Building the custom model from architecture config..\")\n",
    "            # just the 20 layers or 24 layers?\n",
    "            self.backbone = self._load_custom_model()\n",
    "            print(\"custom model loaded\")\n",
    "        \n",
    "        # get fcs\n",
    "        self.fcs = self._create_fcs(grid_size[0], num_bnb_boxes, num_classes)\n",
    "\n",
    "    def _load_custom_model(self):\n",
    "        return get_custom_model(CUSTOM_ARCH)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_bnb_boxes, num_classes):\n",
    "        S, B, C = split_size, num_bnb_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, S * S * (B * 5 + C))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is [B, C, H, W]\n",
    "        # pass it thru the backbone\n",
    "        x = self.backbone(x)\n",
    "        # pass it thru FC layers\n",
    "        x = self.fcs(x).view(-1, self.grid_size[0], self.grid_size[1], self.num_bnb_boxes * 5 + self.num_classes)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv1(use_pretrained=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
